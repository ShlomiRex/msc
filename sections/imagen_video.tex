\section{Imagen-Video}
\label{sec:imagen_video}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/video_synthesis/imagen_video.png}
    \caption{Imagen-Video video samples examples.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/imagen_video/pipeline.png}
    \caption{Imagen-Video cascading pipeline. The text embeddings are injected to all models in the pipeline (its not shown).}
    \label{fig:imagen_video_pipeline}
\end{figure}

Imagen-Video by Google \cite{imagen_video} is a text-to-video cascading diffusion model, based on previous work: Imagen (section \ref{sec:imagen}). It builds on the cascaded nature of Imagen. In total, Imagen-Video has 7 sub-models in a cascading pipeline (figure \ref{fig:imagen_video_pipeline}). Imagen-Video generates high definition $1280\times 768$ videos @ 24 fps, for 5.3 seconds. A big downside of Imagen-Video compared to Video-LDM is that Imagen-Video works in the pixel-space, whereas Video-LDM works in latent space. In addition, Imagen-Video is a much larger model and uses more resources than Video-LDM, however Google is able to achieve good results through massive training scaling.

Like Imagen, Imagen-Video uses the same large frozen text-encoder T5-XXL (section \ref{subsec:t5})in its pipeline.

The benefit of cascading pipeline is the ability to independently train each model, allowing the parallel training of all 7 models. Another benefit is the ability to use the super-resolution models, which are general purpose video super-resolution models, independently on downstream tasks.


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/imagen_video/video_u_net.png}
    \caption{Video U-Net block \cite{video_diffusion_models} used by Imagen-Video. Each frame independently processed by spatial convolution and spatial attention, while a collection of frames are processed by temporal attention and convolution.}
    \label{fig:imagen_video_video_unet}
\end{figure}


Imagen Video also builds on the work of Video U-Net \cite{video_diffusion_models}, which generalizes the 2D diffusion model architecture to 3D in space-time by using temporal attention and 3D convolution layers to capture dependencies between video frames. See figure \ref{fig:imagen_video_video_unet}.













\subsection{Architecture}

Imagen Video has 1 frozen text encoder, 1 base video diffusion model, three SSR (spatial super-resolution) models, and three TSR (temporal super-resolution) models (totaling 7 video diffusion models). Each of the denoising diffusion models $\hat{x_\theta}$ operate on multiple video frames simultaneously.

Whereas typically diffusion models for image generation use a 2D U-Net architecture (spatial attention and convolution), \textbf{Video U-Net} block used by Imagen-Video (figure \ref{fig:imagen_video_video_unet}) generalizes the U-Net to 3D space-time by using temporal attention and convolution to capture dependencies between video frames.

Imagen-Video pipeline is compromised of the following model types:

\begin{itemize}
    \item \textbf{T5-XXL:} As we discussed before, T5-XXL is a text-to-text transformer used to encode (text encoder) the text prompts to embeddings (tokens). These tokens condition all of the 7 diffusion models (not only the base model as shown in figure \ref{fig:imagen_video_video_unet}).
    \item \textbf{Base:} The base diffusion model generate low FPS low resolution video clip, it uses temporal attention.
    \item \textbf{SSR:} The SSR model is a spatial super-resolution (SSR) model that increases the resolution of the input image to higher spatial resolution. Unlike the base model, it uses temporal convolution instead of temporal attention. Like SR3, we first apply bilinear or bicubic interpolation to increase the resolution, then we remove noise (add details) by first concatenating noise $y_t$ to the upsampled image $x$ and then use the U-Net denoising network to learn to denoise the image.
    \item \textbf{TSR:} The TSR model is a temporal super-resolution (TSR) model that increases the temporal resolution of the input video. To achieve this, they repeat the frames or fill blank frames (masked) which increases the amount of frames in the video clip. Unlike the base model, it uses temporal convolution instead of temporal attention.
\end{itemize}


\textbf{Temporal Attention v.s. Temporal Convolution}: The base diffusion model uses temporal attention because they want it to learn to model long term temporal dependencies, whereas the SSR and TSR models use temporal convolution instead, which maintains local temporal consistency. Temporal convolution reduces memory and computation costs over temporal attention, which is critical for the SSR and TSR models, which are applied to high-resolution high-fps videos. This is why temporal attention is used at the beginning of the cascading pipeline.

\textbf{Spatial attention at the beginning of the pipeline}: The base model and the first two SSR models have spatial attention in addition to spatial convolution, because it improves sample fidelity, and the attention mechanism is used in the beginning of the pipeline which requires less compute resources than at the end of the pipeline. For example, the last SSR model in the pipeline is a fully convolutional model.

\textbf{Number of parameters}: Imagen Video consists of 11.6 billion parameters. Each of the model's parameters count is shown in figure \ref{fig:imagen_video_pipeline}.

\textbf{Classifier-free guidance}: CFG is also used in Imagen-Video. They found that it helps the model generate high fidelity samples with respect to the text prompts. Higher guidance weights lead the model to focus more on the text prompt conditioning.

\textbf{Oscillating guidance}: Similar to Imagen, when the guidance weight is too large, the possible range of values of predicted noise is beyond $[-1, 1]$, which causes train-test mismatch. This leads to significant artifacts in the generated videos. \textbf{Dynamic thresholding}, as described in Imagen section, helps to prevent this issue, which dynamically clip the image to the chosen threshold followed by scaling by $s$: \texttt{np.clip(x, -s, s) / s}. However, constant high guidance weight leads to saturation artifacts, especially at high resolutions. The researchers didn't find dynamic thresholding sufficient, therefor they experimented with letting the guidance weight \textbf{oscillate between high and low values} for a certain number of sampling steps. They apply this oscillating dynamic threshold only to the base and first SR models. The generation starts with a high guidance weight to establish a strong alignment with the prompt, then alternates between high and low weights in subsequent steps.



% TODO: Not sure if I should explain this. It only affects the efficiency of sampling, so its not really that important.
\subsection{v-prediction}

In a 2022 paper by Google \cite{salimans2022progressive} the research team introduced a fast sampling method for diffusion models. It allows the diffusion model to sample in a fixed amount of model evaluations, instead of thousands. They presented a way to distill a trained deterministic diffusion sampler (such as \textbf{DDIM} (section \ref{subsec:ddim_sampler})) that takes as half as many sampling steps. Each distillation halves the number of required sampling steps each time.

For example, state-of-the-art samplers that take 8192 steps can be distilled down to as few as 4 steps without significant loss in perceptual quality.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/imagen_video/v_prediction.png}
    \caption{v-prediction: A fast sampling method for diffusion models.}
    \label{fig:v_prediction}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/imagen_video/progressive_distillation.png}
    \caption{Progressive distillation algorithm.}
    \label{fig:progressive_distillation}
\end{figure}

This is achieved by creating a sequence of models, where each model ("\textbf{student}") is trained to mimic a previous, slower model ("\textbf{teacher}") that requires more steps to produce a sample. In each distillation step the student model is initialized with the weights of the teacher model; 

The teacher model uses standard sampling approach, such as DDIM, and then the student model learns to approximate this process in fewer steps.

In figure \ref{fig:progressive_distillation} the trained teacher model $\hat{x}_{\eta} (z_t)$ is the trained diffusion model $\hat{x}_{\theta} (z_t)$, where $z_t$ is the noisy data (we take training data $\mathcal{D}$ and add noise to it). Now we modify this model and make it sample faster by changing the teacher model to the student model, after we distilled the student model. The student model learns to predict $\tilde{x}$ which is the result of two steps of the teacher model (by using two DDIM steps). This way the student model learns to match the two-step output of the previous (teacher) model in one step.

After training on $N$ sampling steps, we repeat this process with $N/2$ steps and so on.












\subsection{Noise conditioning augmentation}

Similar to Imagen, Imagen-Video applied noise conditioning augmentation for all the spatial and temporal diffusion models. They apply Gaussian noise to the conditioning input video during training 

In Imagen the researchers used noise conditioning augmentation to corrupt low-resolution images, and the super-resolution models would be conditioned on the noise level. During training, the corruption noise level ("augmentation level") is chosen randomly. This helps the model to generalize better to various noise levels.

This technique is used primarily in cascading pipeline models, such as Imagen and Imagen-Video. It improves the quality and diversity of samples by changing the noise levels at different stages of training, which helps the model generalize better to various noise levels. The model receives conditioning information about the noise level its handling which guides the model to denoise it, similar to how the model receives timestep embeddings $t$ in the diffusion process; its injected in a similar way.

This technique was introduced in a 2021 paper \cite{cascaded_diffusion_models} by Google.









\subsection{Video-image joint training}

Training on both images and videos data allows the model to learn from both types of data, which improves the model's performance in terms of image and video fidelity. In addition, data of text-video pairs is scarce, and it allows \textbf{knowledge transfer from images to videos}. As a result, this join training enables the model to learn to generate different styles of videos.

Imagen-Video follows the same joint training approach as \cite{video_diffusion_models} (a 2022 paper by Google), in which the model is jointly trained from image and video data. During training, \textbf{individual images are treated as a single frame videos}.

To bypass the temporal components on image data during training, they \textbf{apply a mask to the temporal computation paths}. This masking operation causes the model not to apply temporal operations across frames.



