\section{Imagen-Video}
\label{sec:imagen_video}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/video_synthesis/imagen_video.png}
    \caption{Imagen-Video video samples examples.}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/imagen_video/pipeline.png}
    \caption{Imagen-Video cascading pipeline. The text embeddings are injected to all models in the pipeline (its not shown).}
    \label{fig:imagen_video_pipeline}
\end{figure}

Imagen-Video by Google \cite{imagen_video} is a text-to-video cascading diffusion model, based on previous work: Imagen (section \ref{sec:imagen}). It builds on the cascaded nature of Imagen. In total, Imagen-Video has 7 sub-models in a cascading pipeline (figure \ref{fig:imagen_video_pipeline}). Imagen-Video generates high definition $1280\times 768$ videos @ 24 fps, for 5.3 seconds. A big downside of Imagen-Video compared to Video-LDM is that Imagen-Video works in the pixel-space, whereas Video-LDM works in latent space. In addition, Imagen-Video is a much larger model and uses more resources than Video-LDM, however Google is able to achieve good results through massive training scaling.

Like Imagen, Imagen-Video uses the same large frozen text-encoder T5-XXL (section \ref{subsec:t5})in its pipeline.

The benefit of cascading pipeline is the ability to independently train each model, allowing the parallel training of all 7 models. Another benefit is the ability to use the super-resolution models, which are general purpose video super-resolution models, independently on downstream tasks.


\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/imagen_video/video_u_net.png}
    \caption{Video U-Net block \cite{video_diffusion_models} used by Imagen-Video. Each frame independently processed by spatial convolution and spatial attention, while a collection of frames are processed by temporal attention and convolution.}
    \label{fig:imagen_video_video_unet}
\end{figure}


Imagen Video also builds on the work of Video U-Net \cite{video_diffusion_models}, which generalizes the 2D diffusion model architecture to 3D in space-time by using temporal attention and 3D convolution layers to capture dependencies between video frames. See figure \ref{fig:imagen_video_video_unet}.









\subsection{Architecture}

Imagen Video has 1 frozen text encoder, 1 base video diffusion model, three SSR (spatial super-resolution) models, and three TSR (temporal super-resolution) models (totaling 7 video diffusion models). Each of the denoising diffusion models $\hat{x_\theta}$ operate on multiple video frames simultaneously.

Whereas typically diffusion models for image generation use a 2D U-Net architecture (spatial attention and convolution), \textbf{Video U-Net} block used by Imagen-Video (figure \ref{fig:imagen_video_video_unet}) generalizes the U-Net to 3D space-time by using temporal attention and convolution to capture dependencies between video frames.

Imagen-Video pipeline is compromised of the following model types:

\begin{itemize}
    \item \textbf{T5-XXL:} As we discussed before, T5-XXL is a text-to-text transformer used to encode (text encoder) the text prompts to embeddings (tokens). These tokens condition all of the 7 diffusion models (not only the base model as shown in figure \ref{fig:imagen_video_video_unet}).
    \item \textbf{Base:} The base diffusion model generate low FPS low resolution video clip, it uses temporal attention.
    \item \textbf{SSR:} The SSR model is a spatial super-resolution (SSR) model that increases the resolution of the input image to higher spatial resolution. Unlike the base model, it uses temporal convolution instead of temporal attention. Like SR3, we first apply bilinear or bicubic interpolation to increase the resolution, then we remove noise (add details) by first concatenating noise $y_t$ to the upsampled image $x$ and then use the U-Net denoising network to learn to denoise the image.
    \item \textbf{TSR:} The TSR model is a temporal super-resolution (TSR) model that increases the temporal resolution of the input video. To achieve this, they repeat the frames or fill blank frames (masked) which increases the amount of frames in the video clip. Unlike the base model, it uses temporal convolution instead of temporal attention.
\end{itemize}


\textbf{Temporal Attention v.s. Temporal Convolution}: The base diffusion model uses temporal attention because they want it to learn to model long term temporal dependencies, whereas the SSR and TSR models use temporal convolution instead, which maintains local temporal consistency. Temporal convolution reduces memory and computation costs over temporal attention, which is critical for the SSR and TSR models, which are applied to high-resolution high-fps videos. This is why temporal attention is used at the beginning of the cascading pipeline.

\textbf{Spatial attention at the beginning of the pipeline}: The base model and the first two SSR models have spatial attention in addition to spatial convolution, because it improves sample fidelity, and the attention mechanism is used in the beginning of the pipeline which requires less compute resources than at the end of the pipeline. For example, the last SSR model in the pipeline is a fully convolutional model.

\textbf{Number of parameters}: Imagen Video consists of 11.6 billion parameters. Each of the model's parameters count is shown in figure \ref{fig:imagen_video_pipeline}.





\subsection{v-prediction}

...