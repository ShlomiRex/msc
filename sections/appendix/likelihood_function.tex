\subsection{Likelihood function}
\label{appendix:likelihood_function}

When we talk about the likelihood function we usually mean that in the realm of generative models, our goal is to capture the underlying data distribution, hence we want to generate data ponits with similar likelihood to the training data distribution. 

The formal notion of likelihood function is: $L(x | \theta)$, which reflects the probability of a specific data point ($x$) being generated by the model with its current parameters ($\theta$). However, because we want to maximize this function, we can rewrite the goal as: $\underset{\theta}{\arg\max}\ L(x | \theta)$ (we want to find $\theta$ such that this function is maximum). Furthermore, in many cases, it's computationally more convenient to maximize the \textbf{log-likelihood} function, instead of the likelihood function itself, as the logarithm is a monotonic function (always increasing or decreasing, and therefor the log of the function is also monotonic):

\begin{equation}
\label{eq:mle}
    \hat{\theta}_{MLE} = \underset{\theta}{\arg\max} \ \log L(\mathbf{x} | \theta)
\end{equation}


In this regard, we might not want to directly estimate the likelihood function itself, but techniques like \textbf{maximum likelihood estimation (MLE)} (equation \ref{eq:mle}) can be used to optimize the model's parameters $\theta$ (can be used as a loss function). Calculating MLE directly is not feasible, however, because of intractability (high-integral).

Because of this, we usually use \textbf{Evidence Lower Bound (ELBO)} \ref{sec:elbo} estimation as alternative loss function. Other methods, such as adversarial training is also prominently used in GAN based model training.