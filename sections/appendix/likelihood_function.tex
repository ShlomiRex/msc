\section{Likelihood function}
\label{appendix:likelihood_function}

Likelihood function in the realm of generative models, is often used to capture the underlying data distribution (generate data points with similar likelihood to the training data distribution).

The formal notion of likelihood function is: $L(x | \theta)$, which reflects the probability of a specific data point ($x$) being generated by the model with its current parameters ($\theta$). We want to maximize this term: $\underset{\theta}{\arg\max}\ L(x | \theta)$. In many cases it's computationally more convenient to maximize the \textbf{log-likelihood} function instead, as the logarithm is a monotonic function (always increasing or decreasing, and therefor the log of the function is also monotonic):

\[
    \hat{\theta}_{MLE} = \underset{\theta}{\arg\max} \ \log L(\mathbf{x} | \theta)
\]

Since directly calculating the likelihood is often intractable, methods like \textbf{maximum likelihood estimation (MLE)} optimize $\theta$ indirectly. To address intractability, approaches such as \textbf{Evidence Lower Bound (ELBO)} are used as alternative loss functions. Additionally, adversarial training is widely employed in GAN-based models.
