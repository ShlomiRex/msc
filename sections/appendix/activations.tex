\subsection{Activation functions}
\label{appendix:activations}




\begin{figure*}
    \centering
    \begin{subfigure}[b]{0.4\textwidth}   
        \centering 
        \begin{tikzpicture}
            \begin{axis}[
                axis lines=middle, % Axes through the middle
                axis line style={black, very thick}, % Solid black axes
                xlabel={$x$},
                ylabel={$y$},
                samples=200, % Smooth curves
                domain=-10:10, % Define the range for x
                grid=both,
                grid style={dashed, gray!30}, % Dashed grid lines
                width=5cm, height=5cm, % Size of the plot
                ytick={0, 1},
                xtick={-10, 10}
            ]
                \addplot[thick, red] {1/(1+exp(-x))};
            \end{axis}
        \end{tikzpicture}
        \caption[]%
        {{\small Sigmoid activation function.}}    
        \label{fig:appendix_activations_sigmoid}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}
        \centering
        \begin{tikzpicture}
            \begin{axis}[
                axis lines=middle, % Axes through the middle
                axis line style={black, very thick}, % Solid black axes
                xlabel={$x$},
                ylabel={$y$},
                samples=200, % Smooth curves
                domain=-3:3, % Define the range for x
                grid=both,
                grid style={dashed, gray!30}, % Dashed grid lines
                width=5cm, height=5cm % Size of the plot
                ]
                \addplot[red, thick] {max(0, x)};
            \end{axis}
        \end{tikzpicture}
        \caption[]%
        {{\small ReLU activation function.}}    
        \label{fig:appendix_activations_relu_and_gelu}
    \end{subfigure}
    \vskip\baselineskip
    \begin{subfigure}[b]{0.4\textwidth}  
        \centering 
        \begin{tikzpicture}
            \begin{axis}[
                axis lines=middle, % Axes through the middle
                axis line style={black, very thick}, % Solid black axes
                xlabel={$x$},
                ylabel={$y$},
                samples=200, % Smooth curves
                domain=-6:3, % Define the range for x
                grid=both,
                grid style={dashed, gray!30}, % Dashed grid lines
                width=5cm, height=5cm % Size of the plot
                ]
                \addplot[red, thick] {x / (1 + exp(-x))};
            \end{axis}
        \end{tikzpicture}
        \caption[]%
        {{\small Swish activation function.}}    
        \label{fig:appendix_activations_swish}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.4\textwidth}   
        \centering 
        \begin{tikzpicture}
            \begin{axis}[
                axis lines=middle, % Axes through the middle
                axis line style={black, very thick}, % Solid black axes
                xlabel={$x$},
                ylabel={$y$},
                samples=200, % Smooth curves
                domain=-6:6, % Define the range for x
                grid=both,
                grid style={dashed, gray!30}, % Dashed grid lines
                width=5cm, height=5cm, % Size of the plot
                ytick={-1, 1}
                ]
                \addplot[red, thick] {tanh(x)};
            \end{axis}
        \end{tikzpicture}
        \caption[]%
        {{\small Tanh activation function.}}    
        \label{fig:appendix_activations_tanh}
    \end{subfigure}
    \caption []%
    {\small Common activation functions used in deep learning.} 
\end{figure*}







\subsection*{Sigmoid}

Sigmoid (figure \ref{fig:appendix_activations_sigmoid}) is an activation function used in many neural networks. Its one of the most common activation functions. It normalizes the output in the range [0,1] and is usually used in binary classification or logistic regression tasks. In binary classification task it can be used as the last layer of the neural network, which would output a single probability output. It is defined as:

\begin{equation*}
    \sigma(x) = \frac{1}{1 + e^{-x}}
\end{equation*}









\subsection*{ReLU}

ReLU (Rectified Linear Unit) (figure \ref{fig:appendix_activations_relu_and_gelu}) is an activation function used in many neural networks. It is defined as:

\begin{equation*}
    \text{ReLU}(x) = \max(0, x)
\end{equation*}








\subsection*{Softmax}

Softmax is an activation function, usually used in the output layer of a neural network. Its used to compress multiple input values into a range between [0,1] similarly to sigmoid, but softmax can be thought as a probability distribution, where multiple values are mapped to a probability. Softmax is often used in multi-class classification. It is defined as:

\begin{equation*}
    \text{Softmax}(x_i) = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}}
\end{equation*}

where $x_i$ is the $i$-th element of the input vector $x$ and $n$ is the number of elements in the input vector $x$.






\subsection*{Tanh}

Tanh (figure \ref{fig:appendix_activations_tanh}) is often used in RNNs (Recurrent Neural Networks) and LSTMs (Long Short-Term Memory) gates. Although these models are out of scope in this work, it is still worth mentioning as this activation function is used in many models, such as GANs. The output range is [-1, 1] and its zero centered. It is defined as:

\begin{equation*}
    \tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}}
\end{equation*}








\subsection*{Swish}

Swish (figure \ref{fig:appendix_activations_swish}) is an activation function used in the Imagen \texttt{ResNetBlock}. It was first introduced in a 2017 paper \cite{ramachandran2017swish} by Google Brain team. It is defined as:

\begin{equation}
    \text{Swish}(x) = x \cdot \sigma(x)
    \label{eq:appendix_activations_swish}
\end{equation}

where $\sigma(x)$ is the sigmoid activation function.

Swish is similar to ReLU activation function but it allows smoother gradients during training, and often it outperforms ReLU in some deep learning tasks.

