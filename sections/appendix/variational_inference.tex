\subsection{Variational Inference (VI)}
\label{appendix:variational_inference}

Variational inference (VI) is a technique used to approximate complex posterior distributions in Bayesian inference. Instead of directly maximizing the log-likelihood function, VI aims to minimize the \textbf{Kullback-Leibler (KL) divergence} between an approximate posterior distribution and the true posterior distribution (for instance, learn the distribution of 2D points that are generated by the model, which is estimation of another distribution we want the model to learn, for instance, Gaussian). This is often achieved by minimizing a proxy loss function, such as the \textbf{Evidence Lower Bound (ELBO)} (appendix \ref{appendix:elbo}), which is tractable (can be effectively computed) to optimize. By minimizing the ELBO, VI effectively guides the approximate posterior distribution towards the true posterior distribution.