\subsection{Markov Chains}
\label{appendix:markov_chains}

A Markov chain is a mathematical model that describes a sequence of random events where the probability of the next event depends only on the current state and not on the past events. Unlike fully independent random variables, the probability distribution of a state in a Markov chain depends solely on the state immediately preceding it. This "memoryless" property forms the core principle behind these models â€“ the future state is only influenced by the present state, and the entire history leading up to it becomes irrelevant.

Markov Chains are made up of states and transitions. A state represents a possible condition or situation, and a transition is the movement from one state to another. Each transition has an associated probability that determines the likelihood of moving from one state to another. The probabilities of all possible transitions from a given state must sum to 1.

\begin{figure}
  \centering
  \begin{tikzpicture}[scale=1.2]
    % Define states as nodes
    \node[state, initial] (Sunny) {Sunny};
    \node[state, right=of Sunny] (Rainy) {Rainy};

    % Draw bent arrows with labels
    \draw[->, bend left=40] (Sunny.north) to node[above, midway] {0.8} (Rainy.north);
    \draw[->, bend left=40] (Rainy.south) to node[above, midway] {0.6} (Sunny.south);

    % Add labels for states
    \label{node:sunny} (Sunny);
    \label{node:rainy} (Rainy);
  \end{tikzpicture}
  \caption{A simple Markov chain representing weather transitions.}
  \label{fig:markov_chain}
\end{figure}

In figure \ref{fig:markov_chain} we can see a Markov chain example. It has two states: sunny and rainy. The arrows that connect the states, signify the transition between them, and the label above them is the probability of transition. The defining characteristic of a Markov chain is that the probability of transitioning to the next state depends solely on the current state. The history of previous states has no influence on the future. 