\section{Conclusion}

This work explores recent advancements in deep learning techniques for image and video generation, focusing on the transition from the mature domain of image synthesis to the emerging challenges of video synthesis.

We examined foundational models, including VAEs, GANs, and DMs based models. We explored the state-of-the-art models like VQ-GAN, LDM, and Imagen for image generation and Video-LDM, Imagen-Video, and Make-a-Video for video generation:

\begin{itemize}
    \item \textbf{VQ-VAEs}: Probabilistic models employing latent variables for structured image reconstructions, allowing smooth interpolation in the latent space. VQ-VAE enhances VAEs by introducing vector quantization, discretizing the latent space to facilitate more efficient prior learning and sample generation.
    
    \item \textbf{GANs}: Adversarial models that learn the data distribution by training a generator to produce realistic samples and a discriminator to distinguish between real and generated samples. However, GANs suffer from instability during training due to adversarial loss, leading to mode collapse and convergence issues. As a result, their usage has declined with the rise of diffusion-based methods.
    
    \item \textbf{Stable Diffusion}: Probabilistic model that learns the data distribution by iteratively applying noise to the data and then learns to denoise iteratively. Stable Diffusion is often the basis of most image and video generation models because of its stability and high-fidelity outputs.
    
    \item \textbf{Imagen}: Imagen leverages transformers for text-to-image (T2I) generation, combining their strengths with cascaded diffusion models and super-resolution techniques to achieve state-of-the-art performance. They have shown that the more parameters in the transformer, the better the FID and CLIP evaluation scores.
\end{itemize}

We then shift focus to video synthesis, which builds upon image based techniques to address temporal challenges:

\begin{itemize}
    \item \textbf{VideoGPT}: A transformer-based model that generates videos by predicting future frames conditioned on past frames. It uses VQ-VAE to operate in the latent space instead of the pixel space, which is more compute-efficient.
    
    \item \textbf{Video-LDM}: Takes pre-trained T2I LDM model to video domain by freezing the spatial layers and adding temporal attention and 3D convolution layers in order to fine-tune to video data. It operates in the latent space and incorporates a GAN discriminator to enhance the temporal coherence of generated video.
    
    \item \textbf{Imagen-Video}: a T2V model based on the previous work of Imagen, it extends Imagen to video generation, employing a cascaded diffusion framework with seven diffusion-based models to enhance spatial and temporal resolution.
    
    \item \textbf{Make-a-Video}: A model that is based on the work of DALL-E 2, which employs pseudo-3D convolutions and pseudo-temporal attention to balance computational efficiency with generative quality, addressing the high cost of full temporal attention and 3D convolutions.
\end{itemize}

A big challenge in T2V models is the heavy training cost associated with T2V models, with some tasks requiring the use of hundreds of GPUs. Despite many efforts to reduce the training cost, both the magnitude of dataset and temporal complexity remains a critical concern. More efficient compression of video representations, exploration of effective spatiotemporal blocks and acceleration of training and inference times are essential for the future of long video synthesis.

Autoregressive methods for generating long videos suffer from error accumulation \cite{ouyang2024flexifilm}, resulting in poorer quality in later frames. Moreover, most video generation models currently can only produce videos shorter than 10 seconds.

In summary, the progression from image to video generation models illustrates the evolving capabilities of generative AI. While image synthesis models have reached a level of maturity with highly realistic outputs, video synthesis remains a frontier, requiring innovative approaches to address temporal coherence and computational challenges.