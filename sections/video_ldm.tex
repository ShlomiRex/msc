\section{Video-LDM}
\label{sec:videoldm}

Video-LDM (2023) by NVIDIA \cite{video_ldm} is a T2V model based on LDM. It first trains an image generator on image datasets, then freeze the layers, and then adds temporal layers which helps to align images in temporally consistent manner, transforming it from T2I model to T2V model. Then the model is fine-tuned on video dataset. Freezing the spatial layers transforms the image generation knowledge to the video domain. The model achieves high resolutions of up to $1280\times 2048$.


\subsection*{Mathematical notations}

The LDMs autoencoder compresses the high-dimensional input data $x \in \mathbb{R}^{T \times 3 \times \hat{H} \times \hat{W}}$ where $x \sim p_{\text{data}}$ is a sequence of $T$ RGB frames with height $\hat{H}$ and width $\hat{W}$ to lower-dimensional latent space $z = \mathcal{E} (x) \in \mathbb{R}^{T \times C \times H \times W}$ (where $C$ is the number of latent channels, and $H, W$ are the spatial latent dimensions). The model then learns to reconstruct $x$ with the decoder $\mathcal{D}$: $\hat{x} = \mathcal{D} (\mathcal{E} (x)) \sim x$.

Let us denote the spatial layers $l^i_{\theta}$ where $\theta$ are the model's parameters, and $i$ is the layer index. We then denote the addition of temporal layers as $l^i_{\phi}$ to learn to align individual frames in a temporally consistent manner. We denote $f_{\theta, \phi}$ as the full model (with spatial and temporal layers).








\subsection{Architecture}

\begin{figure}
    \centering
    \includegraphics[width=0.7\textwidth]{images/video_ldm/stack.png}
    \caption{Video-LDM stack \cite{video_ldm}. (1) Keyframe generation. (2) Keyframe filling (keyframe interpolation). (3) Keyframe interpolation is repeated a second time. (4) The latents are decoded to pixel space. (5) Optionally, SR model is applied \cite{video_ldm}.}
    \label{fig:video_ldm_stack}
\end{figure}

The architecture of Video-LDM is similar to Stable Diffusion except for the added temporal layers, and the addition of autoencoder for latent encoding/decoding and the discriminator which helps photorealism in the decoder network. The model's stack is shown in figure \ref{fig:video_ldm_stack}.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/video_ldm/ddpm_sde.png}
    \caption{Generative modeling through stochastic differential equations (SDE) \cite{song2020score}.}
    \label{fig:video_ldm_ode}
\end{figure}

Figure \ref{fig:video_ldm_ode} illustrates the stochastic diffusion process, showing SDE paths that represent sample trajectories. Data distribution $p(x)$ transitions to prior distribution $p_T(x)$ via forward SDE (adding noise) and reverses back to $p(x)$ using reverse SDE. As noise is added, structured data becomes progressively chaotic, converging to a Gaussian prior before reverting to the original distribution. The distribution depicts the data probability, the middle distribution depicts the Gaussian prior, and the right distributions depicts reconstructed data.

\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{images/video_ldm/image_to_video_tuning.png}
    \caption{Transforming image LDM (left) to video LDM (right) via temporal fine-tuning \cite{video_ldm}.}
    \label{fig:video_ldm_image_to_video_tuning}
\end{figure}

In figure \ref{fig:video_ldm_image_to_video_tuning} we see the temporal fine-tuning. We see the stochastic denoising process of diffusion, similar to figure \ref{fig:video_ldm_ode}. We can see that the images on the left side are not temporally aligned, whereas on the right, the images form a coherent video.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{images/video_ldm/enc_dec_denoise_process.png}
    \caption{\textit{Top}: frozen encoder \textit{Right}:  \textit{Bottom}: generative denoising process. \cite{video_ldm}}
    \label{fig:video_ldm_enc_dec_denoise_process}
\end{figure}

In figure \ref{fig:video_ldm_enc_dec_denoise_process} we can see the frozen encoder compresses video frames into 3D latents and the decoder learns to reconstruct them. On the right, we can see the patch-based discriminator $\mathcal{H}$ which is used to increase photorealism of the decoder. It is trained on the adversarial objective which is added to the AE reconstruction score like in PatchGAN \cite{isola2017image} (which tries to classify if $N \times N$ patch is real or fake). On the bottom we can see the generative denoising process, where each embedding corresponds to different image latent in the decoder $\mathcal{D}$.

\begin{figure}
    \centering
    \includegraphics[width=1\textwidth]{images/video_ldm/temporal_layers.png}
    \caption{The researchers turn pre-trained LDM (in gray) into video generator by adding temporal layers (in green) \cite{video_ldm}.}
    \label{fig:video_ldm_spatial_temporal_mixing_layers}
\end{figure}

In figure \ref{fig:video_ldm_spatial_temporal_mixing_layers} we can see that the pre-trained LDM (in gray) is turned into a video generator (on the right) by adding temporal layers $l_\phi$ (in green) that learn to align images in a temporally consistent manner. The spatial layers $l_\theta$ are frozen (in gray) and only the temporal layers are learned (in green). On the right we can see closer look of spatial, temporal layers. One problem arises, though. The pre-trained LDM is trained only on batch, channel, height and width dimensions, but video has one additional dimension: the temporal $t$ dimension. So, in order to deal with the addition of the temporal dimension, they \textbf{rearranged the batch dimension} to be mixed with the temporal dimension (instead of $b$ the batch dimension is now $(b\ t)$) for the next layer: $(b\ t)\ c\ h\ w \rightarrow b\ t\ c\ h\ w$. The $c_S$ represents a context vector used for conditioning the model (explained below in subsection \textbf{context guidance \& masking}).

The input of the spatial layers $l_\theta^i$ is of the dimension $b\ c\ h\ w$ whereas the input dimension of temporal layers is of the dimension $b\ t\ c\ h\ w$. This notation is called \texttt{Einops} (einstein operations) and is further discussed in the Einops paper \cite{einops}. The spatial layers interpret the video as a batch of images by shifting the temporal axis into the batch dimension (the spatial layers treat all $B \cdot T$ encoded video frames in the batch dimension $b$). It is important to note that the rearrange operation doesn't change the tensor values; rather it rearranges the dimensions.

For the 3D convolution (temporal layer), they reshape the input back to video dimensions (they process entire videos in new temporal dimension $t$): 

\[ \underbrace{(b\ t)\ c\ h\ w}_{\text{Output of spatial layer}} \rightarrow \underbrace{b\ c\ t\ h\ w}_{\text{Input to the next Conv3D layer}} \]

After the temporal layers they reshape the input back to fit into the spatial layers like so:

\[ \underbrace{b\ c\ t\ h\ w}_{\text{Output of Conv3D layer}} \rightarrow \underbrace{(b\ t)\ c\ h\ w}_{\text{Input to the next spatial layer}} \]

As for the temporal attention layer (temporal layer), they reshaped the input back to work on the temporal dimension, instead of spatial:

\[ \underbrace{(b\ t)\ c\ h\ w}_{\text{Output of spatial layer}} \rightarrow \underbrace{(b\ h\ w) \ t\ c}_{\text{Input to the next temporal attention layer}} \]

\[ \underbrace{(b\ h\ w) \ t\ c}_{\text{Output of temporal attention layer}} \rightarrow \underbrace{(b\ t)\ c\ h\ w}_{\text{Input to the next spatial layer}} \]

\begin{figure}
    \centering

    \begin{subfigure}{0.3\textwidth}
        \centering
        \scalebox{0.5}{
            \input{figures/video_ldm/rgb.tex}
        }
        \caption{A single frame. The $c\ h\ w$ dimensions.}
    \end{subfigure}

    \begin{subfigure}{0.3\textwidth}
        \centering
        \scalebox{0.5}{
            \input{figures/video_ldm/batch.tex}
        }
        \caption{Multiple frames. The $t\ c\ h\ w$ dimensions.}
    \end{subfigure}

    \begin{subfigure}{0.7\textwidth}
        \centering
        \scalebox{0.5}{
            \input{figures/video_ldm/all_dimensions.tex}
        }
        \caption{All the $b\ t\ c\ h\ w$ dimensions.}
    \end{subfigure}

    \caption{Representation of video dimensions $b\ t\ c\ h\ w$ which corresponds to batch, temporal, channel, height, width dimensions.}
    \label{fig:video_ldm_dimensions}
\end{figure}

We can see visual representation of the dimensions (batch, temporal, channel, height, width) in figure \ref{fig:video_ldm_dimensions}. It's made clear what each dimension represents in the video data.



\textbf{Mixing factor}: after each temporal layer, the output $z'$ is combined with the output of previous spatial layer output $z$ to form a mixing: 

\[ \underbrace{\alpha_\phi^i z_{i} + (1 - \alpha_\phi^i) z_{i}'}_{\text{Mixing factor}} \] 

where $\alpha_\phi^i \in [0, 1]$ and is a learnable parameter. If we set $\alpha = 1$ for each layer skip the temporal score, and we retain the native image generation capability. This mixing operation is similar to CFG (mixing between conditional score and unconditional score).

\textbf{Temporal mixing layers}: two types of temporal mixing layers in use (see figure \ref{fig:video_ldm_spatial_temporal_mixing_layers}) are the \textit{temporal attention} and \textit{Conv3D} layer.

\textbf{Noise scheduler}: Video-LDM uses the same noise scheduler as the underlying image model. In table 6 of the paper, it shows that in all of their models, they use linear noise scheduler.

\textbf{Training objective}: in the training phase only the temporal layers are trained. The objective is the LDM objective (likelihood based, predicting noise in U-Net):

\[ \arg \min_\phi \mathbb{E}_{x \sim p_{\text{data}}, \tau \sim p_{\tau}, \epsilon \sim \mathcal{N} (0, I)} \left[ \left| \left| y - f_{\theta,\phi} (z_{\tau} ; c, \tau) \right| \right|^2_2 \right] \]

where $\tau$ is the diffusion time step, $y$ is the target noise vector. The target is to minimize the difference between predicted noise and ground truth $y$ over all video frames.

\textbf{Adding temporal layers to the decoder:} the researchers add temporal layers to the decoder of the AE which they found this step to be critical for achieving good results. The reason is that the AE is trained on images and flickering artifacts are present in the generated videos (because training on images doesn't teach the model temporal dynamics). So the researchers fine-tune the decoder on video data with a \textbf{patch-wise temporal discriminator built from 3D convolutions}. The encoder, however, remains unchanged.

\textbf{Patch-wise temporal discriminator:} The patch-wise temporal discriminator $\mathcal{H}$, which is built from 3D convolutions, is used to fine-tune the decoder. It's trained on video data and returns "real" or "fake" prediction on patches of the video clip.


\begin{figure}
    \centering
    \scalebox{0.4}{
        \input{figures/video_ldm/keyframes.tex}
    }
    \caption{Keyframe model learns to predict the next latent frame $z$ by context guidance (conditioned on previous frames).}
    \label{fig:video_ldm_keyframes}
\end{figure}

\begin{figure}
    \centering
    \scalebox{0.4}{
        \input{figures/video_ldm/interpolation.tex}
    }
    \caption{Interpolation model learns to fill masked latents $m \circ z$ (L2, L3, L4) in between two keyframes latents (L1, L5) with context guidance.}
    \label{fig:vifdeo_ldm_interpolation}
\end{figure}

\textbf{Context guidance \& Masking}: the model can be conditioned on context information, and is denoted with $c_S$ in figure \ref{fig:video_ldm_spatial_temporal_mixing_layers}. The model is conditioned on the initial set of $S$ context frames, which are the frames at the beginning of the clip. A temporal binary mask is applied to the frames that the model must predict, so the model knows which frames it must predict and which are the context. The latent vector generated by the encoder is concatenated with the binary mask, forming the context guidance $c_S$. In figures \ref{fig:video_ldm_keyframes} and \ref{fig:vifdeo_ldm_interpolation} we can see the context guidance in action: in the keyframe model and the interpolation model.

\textbf{Temporal binary mask \& Higher FPS}: in order to adapt to long duration videos, the researchers trained a $T \rightarrow 4T$ keyframe interpolation model, where for each two keyframes, the model interpolates between them and generates 3 additional frames by using binary masks $m_S$ (0 or 1). The frames are multiplied by the mask and the model learns to predict the missing (masked) frames. This technique achieves higher FPS.

\textbf{LDM Decoder}: the latents are then decoded to the pixel space using the LDM decoder.

\textbf{Upsampler}: to increase the spatial resolution of generated frames, they used a SR model, inspired by cascaded DMs \cite{cascaded_diffusion_models} (the SR3 model \cite{sr3}). In their experiments they used \textbf{pixel-space DM upsampler}, whereas for T2V models they used \textbf{LDM upsampler}. Since upsampling video frames independently would result in poor temporal consistency, they made the SR model video-aware by adding temporal layers and mixing spatial and temporal layers, in similar manner as discussed before.

\textbf{Number of parameters}: the Video-LDM model (except for the CLIP text encoder) consists of 3.1 billion parameters (AE and diffusion models [keyframe generation model, interpolation model, upsamplers]), and only 2.2 billion of these parameters are actually trained:

\begin{itemize}
    \item 84 million parameters in the autoencoder
    \item 865 million parameters in the image backbone LDM, not including CLIP text encoder
    \item 655 million parameters in temporal layers
    \item 354 million parameters in the text encoder (OpenCLIP-ViT/H)
    \item 1.5 million parameters in the interpolation latent diffusion model
\end{itemize}

Compared to Imagen (11.6 billion parameters) and CogVideo (9 billion parameters), Video-LDM is much smaller yet produced high quality videos partly because it works in latent space.

\textbf{DDIM Sampling}: in appendix F of the paper they write that they use the DDIM sampler \cite{ddim} (section \ref{subsec:ddim_sampler}), where the stochastically $\eta$ is varied, as well as the guidance scale.

\textbf{CLIP text encoder}: the text encoder (for T2V conditioning) is a CLIP \cite{openai_clip} based model which is used to generate text embeddings, as we discussed before (section \ref{subsec:clip}).










\subsection{Experiments}

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/video_ldm/videoldm_vs_lvg_on_rds.png}
    \caption{Comparison of Video-LDM and Long Video GAN (LVG) on RDS dataset \cite{video_ldm}.}
    \label{fig:video_ldm_vs_lvg_on_rds}
\end{figure}

In figure \ref{fig:video_ldm_vs_lvg_on_rds} we see comparison of Video-LDM and Long Video GAN (LVG) on RDS dataset. \texttt{Cond.} means the model was conditioned on day/night and crowdedness\footnote{In their experiments, they observed that adding conditional information reduces FID and FVD metrics.}. On the right we see FVD and FID evaluations on different diffusion architectures (pixel-space diffusion model, end-to-end LDM that was not pre-trained on images [which is why the results are bad], and attention-only temporal model). Their model uses 3D temporal convolutions which is better than the attention-only diffusion model.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/video_ldm/rds.png}
    \caption{Real driving videos (RDS) generation samples \cite{video_ldm}.}
    \label{fig:video_ldm_rds}
\end{figure}

In figure \ref{fig:video_ldm_rds} we see real driving videos (RDS) samples. \textit{Top}: night videos. \textit{Middle}: the left red frame is the condition input, while the right is two different generated samples (given a frame, the model can generate the next frames). \textit{Bottom}: they trained a separate bounding-box conditioned LDM for image synthesis only (the RDS dataset has some bounding-box clips), which generates initial frame (in yellow), and then Video-LDM completes the video generation based on this frame.

\begin{figure}
    \centering
    \includegraphics[width=0.5\textwidth]{images/video_ldm/webvid_samples.png}
    \caption{Generated samples trained on WebVid-10M dataset. Prompts: "An astronaut flying in space, 4k, high resolution" and "Milk dripping into a cup of coffee, high definition, 4k" \cite{video_ldm}.}
    \label{fig:video_ldm_webvid_samples}
\end{figure}

We can see text conditioned samples in figure \ref{fig:video_ldm_webvid_samples} on the WebVid-10M dataset at $1280\times 2048$ resolution.

\begin{figure}
    \centering

    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{images/video_ldm/ucf.png}
        \caption{UCF-101 dataset. Video-LDM achieves the best inception score.}
    \end{subfigure}

    \begin{subfigure}{0.8\textwidth}
        \centering
        \includegraphics[width=0.5\textwidth]{images/video_ldm/msr_vtt.png}
        \caption{MSR-VTT dataset. Video-LDM almost achieves the best CLIP-SIM score.}
    \end{subfigure}

    \caption{Video-LDM is compared to other state-of-the-art models in zero-shot setting on UCF-101 and MSR-VTT datasets \cite{video_ldm}.}
    \label{fig:video_ldm_ucf_msr_datasets_metrics}
\end{figure}

In figure \ref{fig:video_ldm_ucf_msr_datasets_metrics} we can see comparison of metrics on UCF-101 and MSR-VTT datasets between Video-LDM and other methods. Video-LDM achieves the best inception score on UCF-101 dataset, and almost achieves the best CLIP-SIM score on MSR-VTT dataset.

The researchers used the following datasets:

\begin{itemize}
    \item \textbf{RDS (Real Driving Videos)}: An in-house dataset of 683,060 videos (up to 8 seconds @ $512\times 1024$ @ 30 fps) with day/night labels and "crowdedness" annotations. Some include car bounding boxes. They used label dropout for CFG and compared Video-LDM to Long Video GAN (LVG) on this dataset (figure \ref{fig:video_ldm_vs_lvg_on_rds}).
    \item \textbf{WebVid-10M}: Contains 10.7M video-caption pairs (52K hours total). They fine-tuned Stable Diffusion spatial layers on frames, added temporal layers, and trained with captions. Upsampled videos to $1280\times 2048$ resolution. Samples are 113 frames (4.7s @ 24 fps or 3.8s @ 30 fps).
    \item \textbf{Mountain Bike Dataset} \cite{brooks2022generating}: Introduced by LVG, it contains 1,202 clips ($\geq 5$ seconds @ 30 fps).
\end{itemize}

\textbf{Evaluation metrics}: they used FID to evaluate quality of individual frames. They also used FVD and human evaluation on video clips and compared to LVG. The human evaluation contained 100 videos @ 4 seconds; given a pair of videos (one from Video-LDM and one from LVG), participants select the most favorable.

\textbf{Video synthesis on RDS dataset}: for video synthesis on RDS dataset, they first generate a single frame using the image LDM, then they run the prediction model on a single frame and generated a sequence of keyframes. Next they performed two steps of the interpolation model which increases the FPS from 1.875 to 7.5 and from 7.5 to 30 fps. And then they ran the temporal upsampler which is run over portions of 8 video frames (the upsampler is also temporally aligned). In their experiments, the researchers successfully generated long, temporally coherent, high-resolution driving videos (up to 5 minutes), trained on the RDS dataset.

