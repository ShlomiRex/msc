\section{VQ-GAN}
\label{sec:vqgan}

Vector Quantized Generative Adversarial Network (VQGAN) \cite{vqgan} is a deep learning model capable of generating high-quality images, with the ability of conditioning (text, images, semantic masks and pose). The architecutre of VQ-GAN is based on previous works: Vector Quantized Variational Autoencoder (VQ-VAE) \cite{vqvae}, transformers \cite{transformer}, and Generative Adversarial Network (GAN) \cite{gan}.

VQ-GAN takes the best of both worlds: the ability to generate high-quality images from GANs and the ability to condition the generation process from VQ-VAE using transformers. The combination of the expressivness of transformers and the inductive bias of CNNs \cite{cnn} in this work showed significant improvements in image generation tasks compared to previous models. Transformers can learn long-range dependencies, whereas CNNs are better fit at learning local features and structures of images.

The model is trained in two stages: first, the VQ-VAE is trained to learn a discrete latent representation of the input data, and then the GAN is trained to generate images from the learned latent representation.