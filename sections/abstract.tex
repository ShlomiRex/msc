\section{Abstract}

This research paper examines recent progress in image and video synthesis using machine learning (ML). While image synthesis has reached a considerable degree of maturity with the development of sophisticated deep learning models, video synthesis continues to present significant research challenges.

Deep generative models, particularly prominent models like DALL-E \cite{dalle}, Sora \cite{sora_website}, Midjourney \cite{midjourney-website}, have gained significant attention due to their ability to produce high-resolution and creative images and videos. In this paper we will focus on 3 image synthesis models and 3 video synthesis models that had a significant contribution to the advancement of the domain. In image synthesis will focus on VQ-GAN \cite{vqgan}, Stable Diffusion \cite{stable_diffusion} and Imagen \cite{imagen}. In video synthesis we will focus on Video-LDM \cite{video_ldm}, Stable Video Diffusion (SVD) \cite{stable_video_diffusion} and Make-a-Video \cite{make_a_video}.

This paper surveys the recent advancements in ML techniques for image and video generation. We explore the fundamental concepts underlying these techniques, focusing on how they learn to create realistic and compelling visual content. We place particular emphasis on the video generation domain, recognizing its immense potential and future applications.

The paper delves into various image generation models, including Variational Autoencoders (VAEs) \cite{vae}, Generative Adversarial Networks (GANs) \cite{gan}, and Diffusion Models (DMs) \cite{ddpm}. We discuss their working principles, strengths, and limitations. Additionally, we explore the use of conditioning to control the output, temporal and spatial cohesion in video synthesis, dataset preparation and learning optimization. Finally, we explore video synthesis techniques that build upon these image generation models, highlighting their unique challenges and interesting points.