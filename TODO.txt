TODO: Remove some of the 'previous works' in Video Synthesis. Its too much. Only keep whatever is best.



In stable diffusion paper, explain the figure 2.

Explain in VQ-GAN paper how they researchers converted conditional information, such as depth map, semantic masks, text, and so on, to tokens. How did images converted to tokens?

Maybe the minGPT encoder converted images to tokens automatically?

Also, you need to explain in the paper what it means that the researchers had to train a new VQ-GAN model for each conditional type.

