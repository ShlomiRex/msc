Read the "High-Resolution Image Synthesis with Latent Diffusion Models" paper, read it carefully and write summorization in onenote

Then take these summorizations and explain the stable diffusion model.

Need to rewrite the introduction to Stable Diffusion. Some of the paragraphs are OK but we need bigger context, put them all together somehow.

Fix appendix: markov chains. We need to add one more image to atleast illustrate it.

In stable diffusion paper, explain the figure 2.




Explain in VQ-GAN paper how they researchers converted conditional information, such as depth map, semantic masks, text, and so on, to tokens. How did images converted to tokens?

Maybe the minGPT encoder converted images to tokens automatically?

Also, you need to explain in the paper what it means that the researchers had to train a new VQ-GAN model for each conditional type.






ב VQ-GAN
להבין איך הם אימנו על conditioning
כלומר למה יצרו מודל חדש עם codebook 
חדש?






לא הבנתי איך ב VQ-GAN
הופכים conditional images
לטוקנים ומעבירים ל transformer
לבדוק את הקוד, לראות איך הופכים תמונות לטוקנים, זה מעניין מאד


