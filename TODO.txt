TODO: Remove some of the 'previous works' in Video Synthesis. Its too much. Only keep whatever is best.

In Video-LDM explain PatchGAN how discriminator works on patches, write it in the appendix

Instead of the second video generator maybe read the paper "CogVideoX: Text-to-Video Diffusion Models with An Expert Transformer" - its much better result

In figure 4 of Video-LDM explain what is the C_s wtf is the speedo-meter in the figure? (resize, conv2d, learned downsampling)

Explain what is "patch-wise temporal discriminator built from 3D convolutions" in Video-LDM, maybe explain PatchGAN?

Move Image-Video figure from Video Synthesis section to Imagen-Video section, when I start to work on Imagen-Video.

Explain Temporal Attention Mechanism in video synthesis and appendix. Its temporal layer, like 3D conv. Its used in Video-LDM.

In diffusion transformers paper explain "adaptive normalization layers" [40] - he says it works like cross-attention that it enables the model to learn conditional information

Video-LDM patch-wise temporal discriminator - architecture of this model is not known? Maybe need to read the rest of the paper

Video-LDM: Temporal binary mask: I don't understand C_s = (m_S \cdot z, m_S). Wtf does that mean

I don't know how the interpolation model works or how the keyframe generation model works in Video-LDM. Do they have different objectives? What are its training regime? How are they different?

The interpolation model in Video-LDM - its input should be 2 latents? How it outputs 3 or more frames/latents? Whats the architecture and training objectives?

In video-LDM they said they used pixel-space DM as upsampler. And also LDM upsampler for text-to-video models. Then whats its architecture? Like why use pixel-space DM and LDM for different models/outputs?

Maybe do section on Long Video GAN (Generating long videos of dynamic scenes) which was state-of-the-art video synthesis model?

CogVideo looks very good, its like an upgraded VideoGPT model, maybe I can do it also? It also has interpolation and keyframe generation

Imagen-Video: explain frame skipping

Imagen-Video: understand what it means: " All models generate an entire block of frames simultaneously â€“ so for instance, our SSR models do not suffer from obvious artifacts that would occur from naively running super-resolution on independent frames"

Imagen-Video: explain Video U-Net block. I don't understand. Read the paper "Video diffusion models".

=========================================================================================================================================================

In stable diffusion paper, explain the figure 2.

Explain in VQ-GAN paper how they researchers converted conditional information, such as depth map, semantic masks, text, and so on, to tokens. How did images converted to tokens?

Maybe the minGPT encoder converted images to tokens automatically?

Also, you need to explain in the paper what it means that the researchers had to train a new VQ-GAN model for each conditional type.

